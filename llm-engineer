#!/usr/bin/env python3
import os
import sys
import re
import json
import asyncio
import difflib
import datetime
import logging
from typing import Optional, Dict, Any
import time
import subprocess
import threading

from dotenv import load_dotenv
from tavily import TavilyClient
from prompt_toolkit import PromptSession
from prompt_toolkit.styles import Style
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.markdown import Markdown
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich.table import Table

import litellm
from litellm import completion, set_verbose

import argparse

class LLMEngineer:
    def __init__(self):
        self.conversation_history = []
        self.file_contents = {}
        self.code_editor_memory = []
        self.code_editor_files = set()
        self.automode = False
        self.running_processes = {}

        # Constants
        self.CONTINUATION_EXIT_PHRASE = "AUTOMODE_COMPLETE"
        self.MAX_CONTINUATION_ITERATIONS = 25
        self.MAX_CONTEXT_TOKENS = 200000
        self.BASE_SYSTEM_PROMPT = self.read_prompt_file('base_system_prompt.md')
        self.AUTOMODE_SYSTEM_PROMPT = self.read_prompt_file('automode_system_prompt.md')

        # Initialize console and other components
        self.console = Console()
        self.tavily = self.initialize_tavily()
        self.SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
        self.CURRENT_DIR = os.getcwd()
        self.DEFAULT_REQUIREMENTS = """
        requests
        numpy
        pandas
        matplotlib
        seaborn
        scikit-learn
        pytz
        """     
        # Load environment variables and set up models
        load_dotenv()
        self.MAIN_MODEL = os.getenv("MAIN_MODEL", "anthropic/claude-3-sonnet-20240620")
        self.TOOLCHECKER_MODEL = os.getenv("TOOLCHECKER_MODEL", "groq/llama3-70b-8192-tool-use-preview")
        self.CODEEDITOR_MODEL = os.getenv("CODEEDITOR_MODEL", "codestral/codestral-latest")
        self.CODEEXECUTION_MODEL = os.getenv("CODEEXECUTION_MODEL", "openai/gpt-4o-mini")

        # Set up token usage tracking
        self.token_usage = {model: {"input": 0, "output": 0} for model in [self.MAIN_MODEL, self.TOOLCHECKER_MODEL, self.CODEEDITOR_MODEL, self.CODEEXECUTION_MODEL]}
        completion.success_callback = [self.track_token_usage]

        self.tools = [
            {
                "type": "function",
                "function": {
                    "name": "create_folder",
                    "description": "Create a new folder at the specified path",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "path": {
                                "type": "string",
                                "description": "The absolute or relative path where the folder should be created"
                            }
                        },
                        "required": ["path"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "create_file",
                    "description": "Create a new file at the specified path with the given content",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "path": {
                                "type": "string",
                                "description": "The absolute or relative path where the file should be created"
                            },
                            "content": {
                                "type": "string",
                                "description": "The content of the file"
                            }
                        },
                        "required": ["path", "content"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "edit_and_apply",
                    "description": "Apply AI-powered improvements to a file based on specific instructions and project context",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "path": {
                                "type": "string",
                                "description": "The absolute or relative path of the file to edit"
                            },
                            "instructions": {
                                "type": "string",
                                "description": "Detailed instructions for the changes to be made"
                            },
                            "project_context": {
                                "type": "string",
                                "description": "Comprehensive context about the project"
                            }
                        },
                        "required": ["path", "instructions", "project_context"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "read_file",
                    "description": "Read the contents of a file at the specified path",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "path": {
                                "type": "string",
                                "description": "The absolute or relative path of the file to read"
                            }
                        },
                        "required": ["path"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "read_multiple_files",
                    "description": "Read the contents of multiple files at the specified paths",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "paths": {
                                "type": "array",
                                "items": {
                                    "type": "string"
                                },
                                "description": "An array of absolute or relative paths of the files to read"
                            }
                        },
                        "required": ["paths"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "list_files",
                    "description": "List all files and directories in the specified folder",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "path": {
                                "type": "string",
                                "description": "The absolute or relative path of the folder to list"
                            }
                        }
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "tavily_search",
                    "description": "Perform a web search using the Tavily API",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "The search query"
                            }
                        },
                        "required": ["query"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "execute_code",
                    "description": "Execute Python code in the isolated code_execution_env environment",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "code": {
                                "type": "string",
                                "description": "The Python code to execute"
                            }
                        },
                        "required": ["code"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "stop_process",
                    "description": "Stop a running process by its ID",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "process_id": {
                                "type": "integer",
                                "description": "The ID of the process to stop"
                            }
                        },
                        "required": ["process_id"]
                    }
                }
            }
        ]

    def read_prompt_file(self, filename):
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read().strip()
        except FileNotFoundError:
            print(f"Warning: {filename} not found. Using empty string as fallback.")
            return ""

    def update_system_prompt(self, current_iteration: Optional[int] = None, max_iterations: Optional[int] = None) -> str:
        chain_of_thought_prompt = """
        Answer the user's request using relevant tools (if they are available). Before calling a tool, do some analysis within <thinking></thinking> tags. First, think about which of the provided tools is the relevant tool to answer the user's request. Second, go through each of the required parameters of the relevant tool and determine if the user has directly provided or given enough information to infer a value. When deciding if the parameter can be inferred, carefully consider all the context to see if it supports a specific value. If all of the required parameters are present or can be reasonably inferred, close the thinking tag and proceed with the tool call. BUT, if one of the values for a required parameter is missing, DO NOT invoke the function (not even with fillers for the missing params) and instead, ask the user to provide the missing parameters. DO NOT ask for more information on optional parameters if it is not provided.

        Do not reflect on the quality of the returned search results in your response.
        """
        
        file_contents_prompt = "\n\nFile Contents:\n"
        for path, content in self.file_contents.items():
            file_contents_prompt += f"\n--- {path} ---\n{content}\n"
        
        if self.automode:
            iteration_info = ""
            if current_iteration is not None and max_iterations is not None:
                iteration_info = f"You are currently on iteration {current_iteration} out of {max_iterations} in automode."
            return self.BASE_SYSTEM_PROMPT + file_contents_prompt + "\n\n" + self.AUTOMODE_SYSTEM_PROMPT.format(iteration_info=iteration_info) + "\n\n" + chain_of_thought_prompt
        else:
            return self.BASE_SYSTEM_PROMPT + file_contents_prompt + "\n\n" + chain_of_thought_prompt

    def initialize_tavily(self):
        tavily_api_key = os.getenv("TAVILY_API_KEY")
        if not tavily_api_key:
            raise ValueError("TAVILY_API_KEY not found in environment variables")
        return TavilyClient(api_key=tavily_api_key)

    def track_token_usage(self, kwargs, completion_response, start_time, end_time):
        model = kwargs['model']
        self.token_usage[model]['input'] += completion_response['usage']['prompt_tokens']
        self.token_usage[model]['output'] += completion_response['usage']['completion_tokens']
        print(f"Token usage for {model}: Input: {self.token_usage[model]['input']}, Output: {self.token_usage[model]['output']}")

    async def get_user_input(self, prompt="You: "):
        style = Style.from_dict({
            'prompt': 'cyan bold',
        })
        session = PromptSession(style=style)
        return await session.prompt_async(prompt, multiline=False)

    def filter_conversation_history(self):
        # Implement a basic filtering strategy
        # This can be improved based on specific requirements
        max_history = 10  # Adjust this value as needed
        return self.conversation_history[-max_history:]

    async def get_tool_arguments(self, tool_name):
        tool = next((t for t in self.tools if t['function']['name'] == tool_name), None)
        if not tool:
            raise ValueError(f"Tool '{tool_name}' not found")

        args = {}
        for param, details in tool['function']['parameters']['properties'].items():
            if param in tool['function']['parameters'].get('required', []):
                args[param] = await self.get_user_input(f"Enter value for '{param}' ({details.get('description', '')}): ")
        return args

    async def chat_with_llm(self, user_input, image_path=None, current_iteration=None, max_iterations=None):
        current_conversation = []
        current_conversation.append({"role": "user", "content": user_input})

        # Filter conversation history to maintain context
        filtered_conversation_history = self.filter_conversation_history()

        # Combine filtered history with current conversation to maintain context
        messages = filtered_conversation_history + current_conversation

        try:
            # Prepend the system message to the messages list
            system_message = {"role": "system", "content": self.update_system_prompt(current_iteration, max_iterations)}
            messages_with_system = [system_message] + messages
            
            response = completion(
                model=self.MAIN_MODEL,
                messages=messages_with_system,
                tools=self.tools,
            )

            if not response or not response.choices:
                self.console.print(Panel("Received an empty response from the AI model.", title="Error", style="bold red"))
                return "I'm sorry, but I received an empty response. Please try again.", False

            assistant_message = response.choices[0].message if response.choices else None
            if not assistant_message:
                self.console.print(Panel("Unable to retrieve the assistant's message.", title="Error", style="bold red"))
                return "I'm sorry, but there was an error processing the response. Please try again.", False

            assistant_response = assistant_message.content or ""
            exit_continuation = self.CONTINUATION_EXIT_PHRASE in assistant_response
            tool_calls = assistant_message.tool_calls if hasattr(assistant_message, 'tool_calls') else []

            self.console.print(Panel(Markdown(assistant_response), title="AI's Response", title_align="left", border_style="blue", expand=False))

            if tool_calls:
                self.console.print(Panel("Tool calls detected", title="Tool Usage", style="bold yellow"))
                self.console.print(Panel(json.dumps([tool_call.model_dump() for tool_call in tool_calls], indent=2), title="Tool Calls", style="cyan"))

            # Display files in context
            if self.file_contents:
                files_in_context = "\n".join(self.file_contents.keys())
            else:
                files_in_context = "No files in context. Read, create, or edit files to add."
            self.console.print(Panel(files_in_context, title="Files in Context", title_align="left", border_style="white", expand=False))

            for tool_call in tool_calls:
                tool_name = tool_call.function.name
                tool_arguments = tool_call.function.arguments
                
                try:
                    tool_input = json.loads(tool_arguments)
                except json.JSONDecodeError:
                    tool_input = {"error": "Failed to parse tool arguments"}

                self.console.print(Panel(f"Tool Used: {tool_name}", style="green"))
                self.console.print(Panel(f"Tool Input: {json.dumps(tool_input, indent=2)}", style="green"))

                tool_result = await self.execute_tool(tool_call.model_dump())
                
                if tool_result["is_error"]:
                    self.console.print(Panel(tool_result["content"], title="Tool Execution Error", style="bold red"))
                else:
                    self.console.print(Panel(tool_result["content"], title_align="left", title="Tool Result", style="green"))

                current_conversation.append({
                    "role": "assistant",
                    "content": None,
                    "tool_calls": [tool_call.model_dump()]
                })

                current_conversation.append({
                    "role": "tool",
                    "content": tool_result["content"],
                    "tool_call_id": tool_call.id
                })

                # Update the file_contents dictionary if applicable
                if tool_name in ['create_file', 'edit_and_apply', 'read_file'] and not tool_result["is_error"]:
                    if 'path' in tool_input:
                        file_path = tool_input['path']
                        if "File contents updated in system prompt" in tool_result["content"] or \
                        "File created and added to system prompt" in tool_result["content"] or \
                        "has been read and stored in the system prompt" in tool_result["content"]:
                            # The file_contents dictionary is already updated in the tool function
                            pass

                messages = filtered_conversation_history + current_conversation

                try:
                    # Prepend the system message to the messages list
                    system_message = {"role": "system", "content": self.update_system_prompt(current_iteration, max_iterations)}
                    messages_with_system = [system_message] + messages
                    
                    tool_response = completion(
                        model=self.TOOLCHECKER_MODEL,
                        messages=messages_with_system,
                        tools=self.tools,
                    )

                    tool_checker_response = tool_response.choices[0].message.content
                    self.console.print(Panel(Markdown(tool_checker_response), title="AI's Response to Tool Result",  title_align="left", border_style="blue", expand=False))
                    assistant_response += "\n\n" + tool_checker_response
                except Exception as e:
                    error_message = f"Error in tool response: {str(e)}"
                    self.console.print(Panel(error_message, title="Error", style="bold red"))
                    assistant_response += f"\n\n{error_message}"

            if assistant_response:
                current_conversation.append({"role": "assistant", "content": assistant_response})

            self.conversation_history = messages + [{"role": "assistant", "content": assistant_response}]

            return assistant_response, exit_continuation

        except Exception as e:
            self.console.print(Panel(f"API Error: {str(e)}", title="API Error", style="bold red"))
            return "I'm sorry, there was an error communicating with the AI. Please try again.", False

    def get_abs_path(self, path):
        if os.path.isabs(path):
            return path
        return os.path.abspath(os.path.join(self.CURRENT_DIR, path))
 
    def create_folder(self, path):
        try:
            os.makedirs(self.get_abs_path(path), exist_ok=True)
            return f"Folder created: {path}"
        except Exception as e:
            return f"Error creating folder: {str(e)}"

    def create_file(self, path, content=""):
        try:
            abs_path = self.get_abs_path(path)
            os.makedirs(os.path.dirname(abs_path), exist_ok=True)
            with open(abs_path, 'w') as f:
                f.write(content)
            self.file_contents[path] = content
            return f"File created and added to system prompt: {path}"
        except Exception as e:
            return f"Error creating file: {str(e)}"

    async def execute_tool(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:
        try:
            function_call = tool_call['function']
            tool_name = function_call['name']
            tool_arguments = function_call['arguments']
            
            # Check if tool_arguments is a string and parse it if necessary
            if isinstance(tool_arguments, str):
                try:
                    tool_input = json.loads(tool_arguments)
                except json.JSONDecodeError:
                    return {
                        "content": f"Error: Failed to parse tool arguments for {tool_name}",
                        "is_error": True
                    }
            else:
                tool_input = tool_arguments

            result = None
            is_error = False

            if tool_name == "create_folder":
                if "path" not in tool_input:
                    raise KeyError("Missing 'path' parameter for create_folder")
                result = self.create_folder(tool_input["path"])
            elif tool_name == "create_file":
                result = self.create_file(tool_input["path"], tool_input.get("content", ""))
            elif tool_name == "edit_and_apply":
                result = await self.edit_and_apply(
                    tool_input["path"],
                    tool_input["instructions"],
                    tool_input["project_context"],
                    is_automode=self.automode
                )
            elif tool_name == "read_file":
                result = self.read_file(tool_input["path"])
            elif tool_name == "read_multiple_files":
                result = self.read_multiple_files(tool_input["paths"])
            elif tool_name == "list_files":
                result = self.list_files(tool_input.get("path", "."))
            elif tool_name == "tavily_search":
                result = self.tavily_search(tool_input["query"])
            elif tool_name == "execute_code":
                execution_result = self.execute_code(tool_input["code"])
                if 'output' in execution_result:
                    analysis = await self.analyze_code_output(tool_input["code"], execution_result['output'])
                    result = f"Execution Output:\n{execution_result['output']}\n\nAnalysis:\n{analysis}"
                else:
                    result = execution_result
            elif tool_name == "stop_process":
                result = self.stop_process(int(tool_input["process_id"]))
            else:
                is_error = True
                result = f"Unknown tool: {tool_name}"

            if isinstance(result, dict) and 'error' in result:
                is_error = True
                result = result['error']
            elif isinstance(result, dict) and 'output' in result:
                result = result['output']

            return {
                "content": result,
                "is_error": is_error
            }
        except KeyError as e:
            error_message = f"Missing required parameter {str(e)} for tool {tool_name}"
            logging.error(error_message)
            return {
                "content": f"Error: {error_message}",
                "is_error": True
            }
        except Exception as e:
            error_message = f"Error executing tool {tool_name}: {str(e)}"
            logging.error(error_message)
            return {
                "content": f"Error: {error_message}",
                "is_error": True
            }

    async def generate_edit_instructions(self, file_path, file_content, instructions, project_context, full_file_contents):
        try:
            # Prepare memory context
            memory_context = "\n".join([f"Memory {i+1}:\n{mem}" for i, mem in enumerate(self.code_editor_memory)])

            # Prepare full file contents context, excluding the file being edited if it's already in code_editor_files
            full_file_contents_context = "\n\n".join([
                f"--- {path} ---\n{content}" for path, content in full_file_contents.items()
                if path != file_path or path not in self.code_editor_files
            ])

            # Read the system prompt from the file
            with open('edit_instructions_system_prompt.md', 'r') as f:
                system_prompt = f.read()

            # Format the system prompt with the specific details
            system_prompt = system_prompt.format(
                file_content=file_content,
                instructions=instructions,
                project_context=project_context,
                memory_context=memory_context,
                full_file_contents_context=full_file_contents_context
            )

            # Make the API call to CODEEDITOR_MODEL
            response = completion(
                model=self.CODEEDITOR_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": "Generate SEARCH/REPLACE blocks for the necessary changes."}
                ],
                max_tokens=8000
            )

            # Parse the response to extract SEARCH/REPLACE blocks
            edit_instructions = self.parse_search_replace_blocks(response.choices[0].message.content)

            # Update code editor memory
            self.code_editor_memory.append(f"Edit Instructions for {file_path}:\n{response.choices[0].message.content}")

            # Add the file to code_editor_files set
            self.code_editor_files.add(file_path)

            return edit_instructions

        except Exception as e:
            self.console.print(f"Error in generating edit instructions: {str(e)}", style="bold red")
            return []  # Return empty list if any exception occurs

    def parse_search_replace_blocks(self, response_text):
        blocks = []
        pattern = r'<SEARCH>\n(.*?)\n</SEARCH>\n<REPLACE>\n(.*?)\n</REPLACE>'
        matches = re.findall(pattern, response_text, re.DOTALL)
        
        for search, replace in matches:
            blocks.append({
                'search': search.strip(),
                'replace': replace.strip()
            })
        
        return json.dumps(blocks)  # Keep returning JSON string

    async def edit_and_apply(self, path, instructions, project_context, is_automode=False, max_retries=3):
        try:
            original_content = self.file_contents.get(path, "")
            if not original_content:
                with open(path, 'r') as file:
                    original_content = file.read()
                self.file_contents[path] = original_content

            for attempt in range(max_retries):
                edit_instructions_json = await self.generate_edit_instructions(path, original_content, instructions, project_context, self.file_contents)
                
                if edit_instructions_json:
                    edit_instructions = json.loads(edit_instructions_json)
                    self.console.print(Panel(f"Attempt {attempt + 1}/{max_retries}: The following SEARCH/REPLACE blocks have been generated:", title="Edit Instructions", style="cyan"))
                    for i, block in enumerate(edit_instructions, 1):
                        self.console.print(f"Block {i}:")
                        self.console.print(Panel(f"SEARCH:\n{block['search']}\n\nREPLACE:\n{block['replace']}", expand=False))

                    edited_content, changes_made, failed_edits = await self.apply_edits(path, edit_instructions, original_content)

                    if changes_made:
                        self.file_contents[path] = edited_content
                        self.console.print(Panel(f"File contents updated in system prompt: {path}", style="green"))
                        
                        if failed_edits:
                            self.console.print(Panel(f"Some edits could not be applied. Retrying...", style="yellow"))
                            instructions += f"\n\nPlease retry the following edits that could not be applied:\n{failed_edits}"
                            original_content = edited_content
                            continue
                        
                        return f"Changes applied to {path}"
                    elif attempt == max_retries - 1:
                        return f"No changes could be applied to {path} after {max_retries} attempts. Please review the edit instructions and try again."
                    else:
                        self.console.print(Panel(f"No changes could be applied in attempt {attempt + 1}. Retrying...", style="yellow"))
                else:
                    return f"No changes suggested for {path}"
            
            return f"Failed to apply changes to {path} after {max_retries} attempts."
        except Exception as e:
            return f"Error editing/applying to file: {str(e)}"

    async def apply_edits(self, file_path, edit_instructions, original_content):
        changes_made = False
        edited_content = original_content
        total_edits = len(edit_instructions)
        failed_edits = []

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            console=self.console
        ) as progress:
            edit_task = progress.add_task("[cyan]Applying edits...", total=total_edits)

            for i, edit in enumerate(edit_instructions, 1):
                search_content = edit['search'].strip()
                replace_content = edit['replace'].strip()
                
                # Use regex to find the content, ignoring leading/trailing whitespace
                pattern = re.compile(re.escape(search_content), re.DOTALL)
                match = pattern.search(edited_content)
                
                if match:
                    # Replace the content, preserving the original whitespace
                    start, end = match.span()
                    # Strip <SEARCH> and <REPLACE> tags from replace_content
                    replace_content_cleaned = re.sub(r'</?SEARCH>|</?REPLACE>', '', replace_content)
                    edited_content = edited_content[:start] + replace_content_cleaned + edited_content[end:]
                    changes_made = True
                    
                    # Display the diff for this edit
                    diff_result = self.generate_diff(search_content, replace_content, file_path)
                    self.console.print(Panel(diff_result, title=f"Changes in {file_path} ({i}/{total_edits})", style="cyan"))
                else:
                    self.console.print(Panel(f"Edit {i}/{total_edits} not applied: content not found", style="yellow"))
                    failed_edits.append(f"Edit {i}: {search_content}")

                progress.update(edit_task, advance=1)

        if not changes_made:
            self.console.print(Panel("No changes were applied. The file content already matches the desired state.", style="green"))
        else:
            # Write the changes to the file
            with open(file_path, 'w') as file:
                file.write(edited_content)
            self.console.print(Panel(f"Changes have been written to {file_path}", style="green"))

        return edited_content, changes_made, "\n".join(failed_edits)

    def generate_diff(self, original, new, path):
        diff = list(difflib.unified_diff(
            original.splitlines(keepends=True),
            new.splitlines(keepends=True),
            fromfile=f"a/{path}",
            tofile=f"b/{path}",
            n=3
        ))

        diff_text = ''.join(diff)
        highlighted_diff = self.highlight_diff(diff_text)

        return highlighted_diff

    def highlight_diff(self, diff_text):
        return Syntax(diff_text, "diff", theme="monokai", line_numbers=True)

    def read_file(self, path):
        try:
            abs_path = self.get_abs_path(path)
            with open(abs_path, 'r') as f:
                content = f.read()
            self.file_contents[path] = content
            return f"File '{path}' has been read and stored in the system prompt."
        except Exception as e:
            return f"Error reading file: {str(e)}"

    def read_multiple_files(self, paths):
        results = []
        for path in paths:
            try:
                abs_path = self.get_abs_path(path)
                with open(abs_path, 'r') as f:
                    content = f.read()
                self.file_contents[path] = content
                results.append(f"File '{path}' has been read and stored in the system prompt.")
            except Exception as e:
                results.append(f"Error reading file '{path}': {str(e)}")
        return "\n".join(results)

    def list_files(self, path="."):
        try:
            abs_path = self.get_abs_path(path)
            files = os.listdir(abs_path)
            return "\n".join(files)
        except Exception as e:
            return f"Error listing files: {str(e)}"

    @staticmethod
    def parse_arguments():
        parser = argparse.ArgumentParser(description="LLM Engineer CLI")
        parser.add_argument("--list-models", action="store_true", help="List the different models assigned to the different roles")
        parser.add_argument("--show-system-message", action="store_true", help="Output the system message")
        parser.add_argument("--show-tools", action="store_true", help="Output the available tools")
        parser.add_argument("--use-tool", help="Use a specific tool")
        parser.add_argument("--test-prompt", help="Pass a prompt and test which tool is picked")
        parser.add_argument("--automode", action="store_true", help="Start in automode")
        parser.add_argument("--prompt", help="Initial prompt for automode. Can be a string or a filename")
        return parser.parse_args()

    def create_code_execution_env(self):
        try:
            env_path = os.path.join(self.SCRIPT_DIR, "code_execution_env")
            venv_path = os.path.join(env_path, "venv")
            drafts_path = os.path.join(env_path, "drafts")

            # Create the main directory
            os.makedirs(env_path, exist_ok=True)

            # Create the virtual environment
            subprocess.run([sys.executable, "-m", "venv", venv_path], check=True)

            # Create the drafts directory
            os.makedirs(drafts_path, exist_ok=True)

            # Create a default requirements.txt file if it doesn't exist
            requirements_path = os.path.join(env_path, "requirements.txt")
            if not os.path.exists(requirements_path):
                with open(requirements_path, 'w') as f:
                    f.write(self.DEFAULT_REQUIREMENTS)

            if os.name == 'nt':  # Windows
                pip_path = os.path.join(venv_path, "Scripts", "pip")
            else:  # macOS and Linux
                pip_path = os.path.join(venv_path, "bin", "pip")

            # Upgrade pip first
            subprocess.run([pip_path, "install", "--upgrade", "pip"], check=True)

            # Install packages one by one
            with open(requirements_path, 'r') as f:
                packages = f.read().splitlines()

            for package in packages:
                if package.strip() and not package.startswith('#'):
                    try:
                        subprocess.run([pip_path, "install", package], check=True)
                    except subprocess.CalledProcessError as e:
                        self.console.print(f"Warning: Failed to install {package}. Error: {str(e)}", style="yellow")

            return "Code execution environment created successfully."
        except subprocess.CalledProcessError as e:
            return f"Error creating code execution environment: {str(e)}"
        except Exception as e:
            return f"Unexpected error creating code execution environment: {str(e)}"

    def execute_code(self, code):
        try:
            if os.name == 'nt':  # Windows
                python_path = os.path.join(self.SCRIPT_DIR, "code_execution_env", "Scripts", "python")
            else:  # macOS and Linux
                python_path = os.path.join(self.SCRIPT_DIR, "code_execution_env", "bin", "python")
            
            process = subprocess.Popen(
                [python_path, "-c", code],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            process_id = id(process)
            self.running_processes[process_id] = process
            
            stdout, stderr = process.communicate(timeout=10)  # 10 second timeout
            
            if process.returncode == 0:
                return {"output": stdout, "process_id": process_id}
            else:
                return {"error": stderr, "process_id": process_id}
        except subprocess.TimeoutExpired:
            return {"output": "Process is still running.", "process_id": process_id}
        except Exception as e:
            return {"error": str(e)}

    async def analyze_code_output(self, code: str, output: str) -> str:
        try:
            response = completion(
                model=self.CODEEXECUTION_MODEL,
                messages=[
                    {"role": "system", "content": "You are an AI assistant specialized in analyzing Python code execution output. Provide insights, explanations, or suggestions based on the code and its output."},
                    {"role": "user", "content": f"Code:\n{code}\n\nOutput:\n{output}\n\nPlease analyze this code execution result."}
                ]
            )
            return response.choices[0].message.content
        except Exception as e:
            logging.error(f"Error in analyzing code output: {str(e)}")
            return f"Error in analyzing code output: {str(e)}"

    def stop_process(self, process_id):
        if process_id in self.running_processes:
            process = self.running_processes[process_id]
            process.terminate()
            try:
                process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                process.kill()
            del self.running_processes[process_id]
            return f"Process {process_id} has been stopped."
        else:
            return f"Process {process_id} not found."

    def tavily_search(self, query):
        try:
            response = self.tavily.qna_search(query=query, search_depth="advanced")
            return response
        except Exception as e:
            return f"Error performing search: {str(e)}"

    def display_tools_friendly(self):
        table = Table(title="Available Tools", show_header=True, header_style="bold magenta")
        table.add_column("Tool Name", style="cyan", no_wrap=True)
        table.add_column("Description", style="green")
        table.add_column("Parameters", style="yellow")

        for tool in self.tools:
            function = tool['function']
            name = function['name']
            description = function['description']
            
            params = function['parameters']['properties']
            param_str = "\n".join([f"{k}: {v.get('description', 'No description')}" for k, v in params.items()])
            
            required = function['parameters'].get('required', [])
            if required:
                param_str += f"\n\nRequired: {', '.join(required)}"

            table.add_row(name, description, param_str)

        self.console.print(Panel(table, title="Tool List", expand=False))

    def parse_goals(self, response):
        return re.findall(r'Goal \d+: (.+)', response)

    async def execute_goals(self, goals):
        for i, goal in enumerate(goals, 1):
            self.console.print(Panel(f"Executing Goal {i}: {goal}", title="Goal Execution", style="bold yellow"))
            response, _ = await self.chat_with_llm(f"Continue working on goal: {goal}")
            if self.CONTINUATION_EXIT_PHRASE in response:
                self.automode = False
                self.console.print(Panel("Exiting automode.", title="Automode", style="bold green"))
                break

    async def run_goals(self, response):
        goals = self.parse_goals(response)
        await self.execute_goals(goals)

    def save_chat(self):
        now = datetime.datetime.now()
        filename = f"Chat_{now.strftime('%Y%m%d_%H%M%S')}.md"
        
        filepath = os.path.join(self.CURRENT_DIR, filename)
        
        formatted_chat = "# LLM Engineer Chat Log\n\n"
        for message in self.conversation_history:
            if message['role'] == 'user':
                formatted_chat += f"## User\n\n{message['content']}\n\n"
            elif message['role'] == 'assistant':
                if isinstance(message['content'], str):
                    formatted_chat += f"## Claude\n\n{message['content']}\n\n"
                elif isinstance(message['content'], list):
                    for content in message['content']:
                        if content['type'] == 'tool_use':
                            formatted_chat += f"### Tool Use: {content['name']}\n\n```json\n{json.dumps(content['input'], indent=2)}\n```\n\n"
                        elif content['type'] == 'text':
                            formatted_chat += f"## Claude\n\n{content['text']}\n\n"
            elif message['role'] == 'user' and isinstance(message['content'], list):
                for content in message['content']:
                    if content['type'] == 'tool_result':
                        formatted_chat += f"### Tool Result\n\n```\n{content['content']}\n```\n\n"
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(formatted_chat)
        
        return filepath

    def reset_code_editor_memory(self):
        self.code_editor_memory = []
        self.console.print(Panel("Code editor memory has been reset.", title="Reset", style="bold green"))

    def reset_conversation(self):
        self.conversation_history = []
        self.file_contents = {}
        self.code_editor_files = set()
        self.reset_code_editor_memory()
        self.console.print(Panel("Conversation history, file contents, code editor memory, and code editor files have been reset.", title="Reset", style="bold green"))

    async def run(self):
        args = self.parse_arguments()

        if args.automode:
            self.automode = True
            initial_prompt = ""
            if args.prompt:
                if os.path.isfile(args.prompt):
                    with open(args.prompt, 'r') as file:
                        initial_prompt = file.read()
                else:
                    initial_prompt = args.prompt
            
            if initial_prompt:
                self.console.print(Panel(f"Starting automode with the following prompt:\n\n{initial_prompt}", title="Automode", style="bold yellow"))
                max_iterations = self.MAX_CONTINUATION_ITERATIONS
                iteration_count = 0
                try:
                    while self.automode and iteration_count < max_iterations:
                        response, exit_continuation = await self.chat_with_llm(initial_prompt if iteration_count == 0 else "Continue with the next step. Or STOP by saying 'AUTOMODE_COMPLETE' if you think you've achieved the results established in the original request.", current_iteration=iteration_count+1, max_iterations=max_iterations)

                        if exit_continuation or self.CONTINUATION_EXIT_PHRASE in response:
                            self.console.print(Panel("Automode completed.", title="Automode", style="green"))
                            self.automode = False
                        else:
                            self.console.print(Panel(f"Continuation iteration {iteration_count + 1} completed. Press Ctrl+C to exit automode.", title="Automode", style="yellow"))
                        iteration_count += 1

                        if iteration_count >= max_iterations:
                            self.console.print(Panel("Max iterations reached. Exiting automode.", title="Automode", style="bold red"))
                            self.automode = False
                except KeyboardInterrupt:
                    self.console.print(Panel("\nAutomode interrupted by user. Exiting automode.", title="Automode", style="bold red"))
                    self.automode = False
                    if self.conversation_history and self.conversation_history[-1]["role"] == "user":
                        self.conversation_history.append({"role": "assistant", "content": "Automode interrupted. How can I assist you further?"})
            else:
                self.console.print(Panel("Automode enabled, but no initial prompt provided. Starting regular chat.", title="Automode", style="bold yellow"))

        if args.list_models:
            self.console.print(Panel(f"""
            MAIN_MODEL: {self.MAIN_MODEL}
            TOOLCHECKER_MODEL: {self.TOOLCHECKER_MODEL}
            CODEEDITOR_MODEL: {self.CODEEDITOR_MODEL}
            CODEEXECUTION_MODEL: {self.CODEEXECUTION_MODEL}
            """, title="Models", style="bold green"))
            return

        if args.show_system_message:
            self.console.print(Panel(self.update_system_prompt(), title="System Message", style="bold green"))
            return

        if args.show_tools:
            self.display_tools_friendly()
            return

        if args.use_tool:
            tool_call = {
                "function": {
                    "name": args.use_tool,
                    "arguments": json.dumps(await self.get_tool_arguments(args.use_tool))
                }
            }
            result = await self.execute_tool(tool_call)
            self.console.print(Panel(json.dumps(result, indent=2), title=f"Tool Result: {args.use_tool}", style="bold green"))
            return

        if args.test_prompt:
            response = completion(
                model=self.MAIN_MODEL,
                messages=[
                    {"role": "system", "content": self.update_system_prompt()},
                    {"role": "user", "content": args.test_prompt}
                ],
                tools=self.tools,
            )
            tool_calls = response.choices[0].message.tool_calls if response.choices[0].message.tool_calls else []
            if tool_calls:
                self.console.print(Panel(f"Tool picked: {tool_calls[0].function.name}", title="Tool Selection", style="bold green"))
            else:
                self.console.print(Panel("No tool was picked for this prompt.", title="Tool Selection", style="bold yellow"))
            return

        self.console.print(Panel("Welcome to the Multi-Model Engineer Chat with LiteLLM integration!", title="Welcome", style="bold green"))
        self.console.print(f"LLM Engineer script location: {self.SCRIPT_DIR}")
        self.console.print(f"Current working directory: {self.CURRENT_DIR}")
        self.console.print("Type 'exit' to end the conversation.")
        self.console.print("Type 'automode [number]' to enter Autonomous mode with a specific number of iterations.")
        self.console.print("Type 'reset' to clear the conversation history.")
        self.console.print("Type 'save chat' to save the conversation to a Markdown file in the current directory.")
        self.console.print("While in automode, press Ctrl+C at any time to exit the automode to return to regular chat.")

        # Create code execution environment
        self.console.print(Panel("Creating code execution environment...", style="yellow"))
        result = self.create_code_execution_env()
        self.console.print(Panel(result, style="green" if "successfully" in result else "red"))

        while True:
            user_input = await self.get_user_input()

            if user_input.lower() == 'exit':
                self.console.print(Panel("Thank you for chatting. Goodbye!", title_align="left", title="Goodbye", style="bold green"))
                break

            if user_input.lower() == 'reset':
                self.reset_conversation()
                continue

            if user_input.lower() == 'save chat':
                filename = self.save_chat()
                self.console.print(Panel(f"Chat saved to {filename}", title="Chat Saved", style="bold green"))
                continue

            if user_input.lower().startswith('automode'):
                try:
                    parts = user_input.split()
                    if len(parts) > 1 and parts[1].isdigit():
                        max_iterations = int(parts[1])
                    else:
                        max_iterations = self.MAX_CONTINUATION_ITERATIONS

                    self.automode = True
                    self.console.print(Panel(f"Entering automode with {max_iterations} iterations. Please provide the goal of the automode.", title_align="left", title="Automode", style="bold yellow"))
                    self.console.print(Panel("Press Ctrl+C at any time to exit the automode loop.", style="bold yellow"))
                    user_input = await self.get_user_input()

                    iteration_count = 0
                    try:
                        while self.automode and iteration_count < max_iterations:
                            response, exit_continuation = await self.chat_with_llm(user_input, current_iteration=iteration_count+1, max_iterations=max_iterations)

                            if exit_continuation or self.CONTINUATION_EXIT_PHRASE in response:
                                self.console.print(Panel("Automode completed.", title_align="left", title="Automode", style="green"))
                                self.automode = False
                            else:
                                self.console.print(Panel(f"Continuation iteration {iteration_count + 1} completed. Press Ctrl+C to exit automode. ", title_align="left", title="Automode", style="yellow"))
                                user_input = "Continue with the next step. Or STOP by saying 'AUTOMODE_COMPLETE' if you think you've achieved the results established in the original request."
                            iteration_count += 1

                            if iteration_count >= max_iterations:
                                self.console.print(Panel("Max iterations reached. Exiting automode.", title_align="left", title="Automode", style="bold red"))
                                self.automode = False
                    except KeyboardInterrupt:
                        self.console.print(Panel("\nAutomode interrupted by user. Exiting automode.", title_align="left", title="Automode", style="bold red"))
                        self.automode = False
                        if self.conversation_history and self.conversation_history[-1]["role"] == "user":
                            self.conversation_history.append({"role": "assistant", "content": "Automode interrupted. How can I assist you further?"})
                except KeyboardInterrupt:
                    self.console.print(Panel("\nAutomode interrupted by user. Exiting automode.", title_align="left", title="Automode", style="bold red"))
                    self.automode = False
                    if self.conversation_history and self.conversation_history[-1]["role"] == "user":
                        self.conversation_history.append({"role": "assistant", "content": "Automode interrupted. How can I assist you further?"})

                self.console.print(Panel("Exited automode. Returning to regular chat.", style="green"))
            else:
                response, _ = await self.chat_with_llm(user_input)


if __name__ == "__main__":
    llm_engineer = LLMEngineer()
    asyncio.run(llm_engineer.run())
